{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TaxonomyLLM: Enhancing Data Classification with LLMs\n",
    "\n",
    "Welcome to this detailed exploration of TaxonomyLLM, a cutting-edge approach to automating taxonomy generation using large language models (LLMs). In this notebook, we delve into the complexities and innovations of TaxonomyLLM, showcasing its significant role in transforming data classification in big data environments.\n",
    "\n",
    "## Business Value of Automated Taxonomy\n",
    "In today's data-driven world, efficiently organizing and classifying large volumes of data is crucial. Manual taxonomy generation is time-consuming and prone to errors, making it unscalable in large data environments. TaxonomyLLM addresses these challenges by automating the process, ensuring:\n",
    "- **Efficiency:** Rapidly processes vast datasets.\n",
    "- **Accuracy:** Reduces human error in data tagging.\n",
    "- **Scalability:** Adapts to growing data without additional manual effort.\n",
    "\n",
    "This notebook will guide you through the theoretical concepts, practical implementation, and the impressive potential of TaxonomyLLM in the realm of data science and artificial intelligence.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c7a29315fae849e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Theoretical Background and Concepts\n",
    "\n",
    "### Understanding Large Language Models (LLMs)\n",
    "LLMs have revolutionized the way we process language data. They are designed to understand, generate, and interpret human language in a way that is both contextually relevant and meaningful.\n",
    "\n",
    "#### Mathematical Abstractions in LLMs\n",
    "- **Vector Spaces:** LLMs represent words and phrases as vectors in high-dimensional spaces.\n",
    "- **Neural Network Architectures:** Discuss architectures like transformers that are pivotal in LLMs.\n",
    "- **Attention Mechanisms:** Explain how attention mechanisms help LLMs focus on relevant parts of the input data.\n",
    "\n",
    "### Topological Attention Mechanisms\n",
    "These mechanisms are crucial for understanding the relationships and structures within data. They enable LLMs to disentangle complex data patterns and extract meaningful taxonomy tags.\n",
    "\n",
    "#### Diagram: LLM Architecture\n",
    "\n",
    "<img src=\"design.png\" alt=\"Image Description\" width=\"500\" height=\"500\">\n",
    "\n",
    "### Relevance in Automated Taxonomy Generation\n",
    "Automated taxonomy generation with LLMs bridges the gap between unstructured data and structured classification, making it a pivotal tool in data management.\n",
    "\n",
    "### Neural network\n",
    "\n",
    "<img src=\"taxonomy_neural_network.png\" alt=\"Image Description\" width=\"500\" height=\"500\">\n",
    "\n",
    "\n",
    "- **Input Data**: Represents the input SQL text.\n",
    "- **Embedding Layer**: Corresponds to the embedding layer within the `SchemaEncoder`.\n",
    "- **Flatten Layer**: Corresponds to the flatten layer within the `SchemaEncoder`.\n",
    "- **RDF Generation**: Represents the `TaxonomyDecoder` layer responsible for generating RDF data.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21e816cc4980e04d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Project Setup and Dependencies"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72f58286dca4fc"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-21T14:25:16.701273Z",
     "start_time": "2024-01-21T14:25:09.166332Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-21 14:25:11.845769: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.15.0\n"
     ]
    }
   ],
   "source": [
    "# Project Setup and Dependencies\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Embedding, Flatten\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from rdflib import Namespace, Graph, URIRef, Literal\n",
    "from io import StringIO\n",
    "\n",
    "# Environment setup\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Display TensorFlow version for reference\n",
    "print(\"TensorFlow version:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Parsing SQL Schema"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8335ae2ec3c28037"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Columns: ['id', 'name', 'category', 'price']\n"
     ]
    }
   ],
   "source": [
    "# Parsing the SQL Schema\n",
    "sql_text = \"\"\"\n",
    "CREATE TABLE Products (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    name TEXT, \n",
    "    category TEXT,\n",
    "    price DECIMAL  \n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Extracting column names\n",
    "columns = [line.split()[0] for line in sql_text.split('\\n') if line.strip() and line.strip().split()[0] in ['id', 'name', 'category', 'price']]\n",
    "\n",
    "# Display extracted columns\n",
    "print(\"Extracted Columns:\", columns)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T14:25:16.708325Z",
     "start_time": "2024-01-21T14:25:16.705057Z"
    }
   },
   "id": "e1f3f451bd6cb506",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Schema Encoding and Mathematical Concepts\n",
    "\n",
    "### Mathematical Explanation of Embeddings in SchemaEncoder\n",
    "\n",
    "### What are Embeddings?\n",
    "\n",
    "Embeddings are a type of representation that maps discrete, categorical data, such as words in text, into vectors of real numbers. This is crucial in the field of machine learning, particularly when dealing with types of data that are not inherently numerical.\n",
    "\n",
    "In the context of TaxonomyLLM, embeddings are used to convert components of an SQL schema into a numerical format. This allows the subsequent neural network layers to process and learn from these data.\n",
    "\n",
    "### Mathematical Representation of Embeddings\n",
    "\n",
    "Consider a vocabulary $V$ of size $N$, where each word (or in our case, schema component) is represented by a unique integer in the range $[1, N]$. An embedding is then a function $f: V \\rightarrow \\mathbb{R}^d$, where $d$ is the dimensionality of the embedding space. This function maps each word to a $d$-dimensional vector.\n",
    "\n",
    "### Embedding Matrix\n",
    "\n",
    "The embeddings for all words in the vocabulary are stored in a matrix $E$ of size $N \\times d$, where each row $i$ of the matrix corresponds to the $d$-dimensional embedding of the $i$-th word in the vocabulary.\n",
    "\n",
    "If the integer representation of a word is given by $w_i$, its embedding is found by:\n",
    "\n",
    "$$ \\text{Embedding of } w_i = E_{w_i} $$\n",
    "\n",
    "### Transforming Categorical Data into Numerical Format\n",
    "\n",
    "In our SQL schema example, each component of the schema (like 'id', 'name', 'category', 'price') is first assigned a unique integer. These integers are then used to look up the corresponding embeddings in the embedding matrix $E$.\n",
    "\n",
    "For a given schema component $c$, its integer representation $w_c$ is used to fetch its embedding:\n",
    "\n",
    "$$ \\text{Numerical Representation of } c = E_{w_c} $$\n",
    "\n",
    "This process effectively transforms the categorical schema components into a format that neural network models can process, enabling them to learn and make predictions based on this data.\n",
    "\n",
    "### Importance in Neural Networks\n",
    "\n",
    "Embeddings are particularly important in neural network models because they allow these models to capture more information about the data, such as relationships and similarities between different schema components. By learning dense representations in a high-dimensional space, neural networks can discern subtle patterns and correlations in the data, which would be difficult or impossible to capture using traditional one-hot encoding methods.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "In summary, embeddings are a powerful tool in machine learning, providing a way to convert categorical data into a rich, numerical format suitable for processing by neural networks. In TaxonomyLLM's `SchemaEncoder`, embeddings play a crucial role in enabling the model to effectively learn from and make predictions based on SQL schema data.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8615bc30e827a16"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Schema Encoding and Mathematical Foundations\n",
    "class SchemaEncoder(Layer):\n",
    "    def __init__(self):\n",
    "        super(SchemaEncoder, self).__init__()\n",
    "        self.embedding = Embedding(1000, 32)\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "    def call(self, seq):\n",
    "        x = self.embedding(seq)\n",
    "        return self.flatten(x)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T14:25:16.714003Z",
     "start_time": "2024-01-21T14:25:16.711826Z"
    }
   },
   "id": "6602d365ea06937d",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Taxonomy Generation with RDF\n",
    "\n",
    "## Explanation of RDF Generation\n",
    "\n",
    "### Understanding RDF in the Context of Relational Data\n",
    "\n",
    "The Resource Description Framework (RDF) is a standard for representing information about resources in the World Wide Web. RDF fundamentally represents information in the form of statements about resources, expressed as a triple structure comprising a subject, predicate, and object.\n",
    "\n",
    "#### Mathematical Representation of RDF\n",
    "\n",
    "RDF can be mathematically thought of as a set of triples, each of which is an ordered tuple \\( (s, p, o) \\), where:\n",
    "- \\( s \\) is the subject, representing the resource from which a property emanates.\n",
    "- \\( p \\) is the predicate, representing the property or relationship between the subject and the object.\n",
    "- \\( o \\) is the object, representing the resource or value that is linked to the subject.\n",
    "\n",
    "In formal terms, if we have a set of subjects \\( S \\), a set of predicates \\( P \\), and a set of objects \\( O \\), an RDF triple is an element of the Cartesian product \\( S \\times P \\times O \\).\n",
    "\n",
    "### Significance of Each Component\n",
    "\n",
    "- **Subject (s):** The subject in an RDF triple is the 'thing' or resource about which a statement is made. It can be an entity, a concept, or anything identifiable in the domain of discourse.\n",
    "\n",
    "- **Predicate (p):** The predicate denotes characteristics of the resource or a relationship between the subject and object. Predicates define the type of interaction or the nature of the association between the subject and object.\n",
    "\n",
    "- **Object (o):** The object is the value of the property or the resource that is in relation with the subject. It can be another resource or a literal value (like a string, number, date, etc.).\n",
    "\n",
    "### RDF and Relational Data\n",
    "\n",
    "In the context of relational data, such as an SQL schema, RDF provides a flexible way to represent the relationships and attributes of the data. For instance, each column in a database table can be a subject, with predicates describing its characteristics (like data type, constraints) and the relationships with other entities.\n",
    "\n",
    "### Diagrams Illustrating RDF Triples\n",
    "\n",
    "To illustrate, consider a simple RDF representation of a database table named 'Products':\n",
    "\n",
    "- Subject: Product ID (e.g., 'Product123')\n",
    "- Predicate: 'hasName'\n",
    "- Object: 'Gadget Pro'\n",
    "\n",
    "This can be visually represented as:\n",
    "\n",
    "```\n",
    "[Product123] -- hasName --> ['Gadget Pro']\n",
    "```\n",
    "\n",
    "Similarly, other properties and relationships can be represented, forming a network of interconnected RDF triples that comprehensively describe the schema and its data.\n",
    "\n",
    "### Mathematical Foundation in RDF Generation\n",
    "\n",
    "The process of generating RDF from a relational schema involves mapping the elements of the database (tables, columns, rows) into RDF triples. This involves a series of functions that translate the relational schema components into the RDF format. The mathematical abstraction here can be viewed as a transformation function:\n",
    "\n",
    "\\[ f: (Database Schema) \\rightarrow (RDF Triples) \\]\n",
    "\n",
    "This function takes the components of the database schema as input and outputs a set of RDF triples representing the schema's structure and relationships.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "RDF provides a robust and flexible framework for representing relational data, especially in contexts where the relationships and attributes of the data need to be described in a machine-readable format. In the realm of machine learning and AI, such as in the TaxonomyLLM project, understanding and generating RDF is crucial for effectively processing and interpreting relational data structures.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18f39f0fbad91d07"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Taxonomy Generation with RDF\n",
    "class TaxonomyDecoder(Layer):\n",
    "    def call(self, vectors):\n",
    "        return tf.py_function(func=generate_rdf, inp=[vectors], Tout=tf.string)\n",
    "\n",
    "def generate_rdf(vectors):\n",
    "    graph = Graph()\n",
    "    base_uri = \"http://example.org/\"\n",
    "    uri = Namespace(base_uri)\n",
    "\n",
    "    for i, column in enumerate(columns):\n",
    "        subject = URIRef(f\"{base_uri}{column}\")\n",
    "        predicate = URIRef(f\"{base_uri}hasDataType\")\n",
    "        object = URIRef(f\"{base_uri}{random.choice(['INTEGER', 'TEXT', 'DECIMAL'])}\")\n",
    "        graph.add((subject, predicate, object))\n",
    "        graph.add((subject, uri.label, Literal(f\"Column: {column}\")))\n",
    "        graph.add((predicate, uri.label, Literal(\"has data type\")))\n",
    "        graph.add((object, uri.label, Literal(f\"Data Type: {object.split('/')[-1]}\")))\n",
    "\n",
    "    return graph.serialize(format=\"ttl\").encode('utf8')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T14:25:16.746130Z",
     "start_time": "2024-01-21T14:25:16.721241Z"
    }
   },
   "id": "b20ade98821a4b4d",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Results Visualization\n",
    "\n",
    "### Visualization of RDF Triples\n",
    "\n",
    "To better understand the structure and relationships defined by RDF triples, a visual representation can be extremely helpful. Below is a diagram illustrating the RDF triples derived from our SQL schema example.\n",
    "\n",
    "![RDF Triples Visualization](rdf_generated.png)\n",
    "\n",
    "*Figure: An illustrative diagram showing RDF triples, with each node representing a subject, predicate, or object, and the edges representing the relationships between them.*\n",
    "\n",
    "In this visualization:\n",
    "- Each **node** represents either a subject, predicate, or object.\n",
    "- Each **directed edge** indicates the relationship from subject to object through a predicate.\n",
    "\n",
    "This graphical representation aids in comprehensively understanding the complex interrelations within RDF data, which is especially useful in large-scale data environments.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d79ded1eb53a24"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated RDF Graph:\n",
      "@prefix ns1: <http://example.org/> .\n",
      "\n",
      "ns1:category ns1:hasDataType ns1:TEXT ;\n",
      "    ns1:label \"Column: category\" .\n",
      "\n",
      "ns1:hasDataType ns1:label \"has data type\" .\n",
      "\n",
      "ns1:id ns1:hasDataType ns1:DECIMAL ;\n",
      "    ns1:label \"Column: id\" .\n",
      "\n",
      "ns1:name ns1:hasDataType ns1:TEXT ;\n",
      "    ns1:label \"Column: name\" .\n",
      "\n",
      "ns1:price ns1:hasDataType ns1:INTEGER ;\n",
      "    ns1:label \"Column: price\" .\n",
      "\n",
      "ns1:DECIMAL ns1:label \"Data Type: DECIMAL\" .\n",
      "\n",
      "ns1:INTEGER ns1:label \"Data Type: INTEGER\" .\n",
      "\n",
      "ns1:TEXT ns1:label \"Data Type: TEXT\" .\n"
     ]
    }
   ],
   "source": [
    "# Results Visualization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([sql_text])\n",
    "sql_seq = tokenizer.texts_to_sequences([sql_text])[0]\n",
    "sql_seq = tf.expand_dims(sql_seq, 0)\n",
    "\n",
    "encoder = SchemaEncoder()\n",
    "vectors = encoder(sql_seq)\n",
    "\n",
    "decoder = TaxonomyDecoder()\n",
    "rdf_graph_serialized = decoder(vectors)\n",
    "\n",
    "# Display the generated RDF graph\n",
    "print(\"Generated RDF Graph:\\n\" + rdf_graph_serialized.numpy().decode(\"utf-8\"))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T14:25:16.786555Z",
     "start_time": "2024-01-21T14:25:16.744707Z"
    }
   },
   "id": "baa1b4c53c44e0b6",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Discussion and Implications\n",
    "\n",
    "TaxonomyLLM's approach opens up new avenues in data science and AI. By automating taxonomy generation, it finds applications in various domains like e-commerce, content management, and more. This section can delve into the potential impact of TaxonomyLLM across different industries, discussing its versatility and adaptability.\n",
    "\n",
    "### Future Research Directions\n",
    "- Expansion to more complex schemas.\n",
    "- Integration with larger and more diverse datasets.\n",
    "- Exploration of other neural network architectures for enhanced performance.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e916215cf93d5fc0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we have explored TaxonomyLLM, a novel approach to automating taxonomy generation using LLMs. We have discussed its theoretical underpinnings, practical implementation, and potential impact on data management and AI. TaxonomyLLM represents a significant step forward in making data classification more efficient, accurate, and scalable.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6e61caf135ec59a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f8356011fe29761b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
